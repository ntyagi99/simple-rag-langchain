{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Notebook 02: Document Loaders\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Load documents from **PDF files** using PyPDFLoader\n",
    "2. Load structured data from **CSV files**\n",
    "3. Load JSON data from **API responses** or files\n",
    "4. Scrape and load content from **web pages** (HTML)\n",
    "5. Load **text files** and **markdown files**\n",
    "6. **Batch process** multiple files using DirectoryLoader\n",
    "7. Understand Document object structure\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Table of Contents\n",
    "\n",
    "1. [Why Document Loaders?](#why-loaders)\n",
    "2. [Document Object Structure](#document-structure)\n",
    "3. [Loading PDF Files](#pdf-loading)\n",
    "4. [Loading CSV Files](#csv-loading)\n",
    "5. [Loading JSON Files](#json-loading)\n",
    "6. [Loading Web Pages (HTML)](#html-loading)\n",
    "7. [Loading Text and Markdown Files](#text-loading)\n",
    "8. [Batch Loading with DirectoryLoader](#batch-loading)\n",
    "9. [Comparison Table](#comparison)\n",
    "10. [Best Practices](#best-practices)\n",
    "11. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-loaders\"></a>\n",
    "## 1. Why Document Loaders? ü§î\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**Document Loaders** are tools that help you convert files (PDFs, CSVs, web pages, etc.) into **Document objects** that LangChain can work with.\n",
    "\n",
    "Think of them as **translators**:\n",
    "- **Input**: Files in various formats (PDF, CSV, JSON, HTML)\n",
    "- **Output**: Standardized Document objects with text content and metadata\n",
    "\n",
    "### Why is this important?\n",
    "\n",
    "Every RAG application needs to:\n",
    "1. üì• **Load** data from various sources\n",
    "2. üîÑ **Convert** it to a standard format\n",
    "3. üìä **Extract** metadata (source, page number, etc.)\n",
    "4. üéØ **Prepare** it for embedding and retrieval\n",
    "\n",
    "Document Loaders handle all of this automatically!\n",
    "\n",
    "### üéì INTERMEDIATE\n",
    "\n",
    "All document loaders in LangChain implement the same interface:\n",
    "- `.load()`: Load all documents at once (returns list[Document])\n",
    "- `.lazy_load()`: Load documents one at a time (generator, memory efficient)\n",
    "\n",
    "This consistency makes it easy to switch between different data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "print(\"‚úÖ Environment loaded\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Sample data directory exists: {Path('sample_data').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"document-structure\"></a>\n",
    "## 2. Document Object Structure üìÑ\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "Every Document has two main parts:\n",
    "1. **page_content**: The actual text (string)\n",
    "2. **metadata**: Information about the document (dictionary)\n",
    "\n",
    "Think of it like a book:\n",
    "- **page_content** = The story\n",
    "- **metadata** = The cover information (title, author, page number, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a sample document\n",
    "doc = Document(\n",
    "    page_content=\"This is the actual content of the document. It contains the text we want to process.\",\n",
    "    metadata={\n",
    "        \"source\": \"example.pdf\",\n",
    "        \"page\": 1,\n",
    "        \"author\": \"John Doe\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Inspect the document\n",
    "print(\"üìÑ Document Structure:\")\n",
    "print(f\"\\nType: {type(doc)}\")\n",
    "print(f\"\\nContent (first 100 chars): {doc.page_content[:100]}...\")\n",
    "print(f\"\\nMetadata: {doc.metadata}\")\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "print(f\"Page Number: {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pdf-loading\"></a>\n",
    "## 3. Loading PDF Files üìï\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**PyPDFLoader** is used to load PDF files. It:\n",
    "- Extracts text from each page\n",
    "- Creates one Document per page\n",
    "- Automatically adds source and page number to metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Loading a Single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the \"Attention is All You Need\" paper (if it exists)\n",
    "pdf_path = \"attention.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(f\"Loading PDF: {pdf_path}\")\n",
    "    print(\"‚è≥ This may take a moment...\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} pages\\n\")\n",
    "    \n",
    "    # Inspect first page\n",
    "    print(\"üìÑ First Page:\")\n",
    "    print(f\"   Content (first 200 chars): {documents[0].page_content[:200]}...\")\n",
    "    print(f\"\\n   Metadata: {documents[0].metadata}\")\n",
    "    \n",
    "    # Inspect last page\n",
    "    print(f\"\\nüìÑ Last Page (page {len(documents)}):\")\n",
    "    print(f\"   Content (first 200 chars): {documents[-1].page_content[:200]}...\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå PDF not found: {pdf_path}\")\n",
    "    print(\"   Make sure the file exists in the project root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Lazy Loading for Large PDFs\n",
    "\n",
    "For very large PDFs, use `.lazy_load()` to process one page at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy loading example\n",
    "if Path(pdf_path).exists():\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    print(\"üîÑ Lazy loading pages (memory efficient):\")\n",
    "    \n",
    "    # Process first 3 pages only\n",
    "    for i, page in enumerate(loader.lazy_load()):\n",
    "        if i >= 3:  # Only process first 3 pages for demo\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nPage {i+1}:\")\n",
    "        print(f\"  Length: {len(page.page_content)} characters\")\n",
    "        print(f\"  Preview: {page.page_content[:100]}...\")\n",
    "    \n",
    "    print(\"\\nüí° Tip: Use lazy_load() for PDFs > 100 pages to save memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Loading Multiple PDFs from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all PDFs from the pdfs/ directory\n",
    "pdf_directory = \"pdfs\"\n",
    "\n",
    "if Path(pdf_directory).exists():\n",
    "    print(f\"üìÇ Loading PDFs from: {pdf_directory}/\\n\")\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"  - {pdf_file.name}\")\n",
    "        \n",
    "        # Load each PDF\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        \n",
    "        print(f\"    ‚úÖ Loaded {len(docs)} pages\")\n",
    "    \n",
    "    print(f\"\\nüìä Total: {len(all_documents)} pages from {len(pdf_files)} PDFs\")\n",
    "    \n",
    "    # Show unique sources\n",
    "    sources = set(doc.metadata['source'] for doc in all_documents)\n",
    "    print(f\"\\nSources:\")\n",
    "    for source in sources:\n",
    "        print(f\"  - {Path(source).name}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {pdf_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"csv-loading\"></a>\n",
    "## 4. Loading CSV Files üìä\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**CSVLoader** converts each row of a CSV file into a separate Document.\n",
    "\n",
    "**Use cases:**\n",
    "- Product catalogs\n",
    "- FAQ databases\n",
    "- Customer records\n",
    "- Any tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Load the products CSV\n",
    "csv_path = \"sample_data/products.csv\"\n",
    "\n",
    "if Path(csv_path).exists():\n",
    "    print(f\"Loading CSV: {csv_path}\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_path,\n",
    "        source_column=\"product_name\"  # Which column to use as source in metadata\n",
    "    )\n",
    "    \n",
    "    # Load all rows\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} products\\n\")\n",
    "    \n",
    "    # Inspect first 3 products\n",
    "    for i, doc in enumerate(documents[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Product {i}:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(doc.page_content)\n",
    "        print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "        print(f\"Row: {doc.metadata.get('row', 'N/A')}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"... and {len(documents) - 3} more products\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå CSV not found: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Custom CSV Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(csv_path).exists():\n",
    "    # Advanced CSV loading with custom configuration\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_path,\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'fieldnames': None,  # Use first row as headers\n",
    "        },\n",
    "        source_column=\"product_id\"  # Use product_id as source\n",
    "    )\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Show how metadata is different\n",
    "    print(\"üìä CSV with custom configuration:\\n\")\n",
    "    print(f\"First document source: {docs[0].metadata['source']}\")\n",
    "    print(f\"Content preview:\\n{docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"json-loading\"></a>\n",
    "## 5. Loading JSON Files üîß\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**JSONLoader** extracts data from JSON files using **jq** syntax (a query language for JSON).\n",
    "\n",
    "**Common use cases:**\n",
    "- API responses\n",
    "- Configuration files\n",
    "- Structured data exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load the API response JSON\n",
    "json_path = \"sample_data/api_response.json\"\n",
    "\n",
    "if Path(json_path).exists():\n",
    "    print(f\"Loading JSON: {json_path}\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    # jq_schema tells us where to find the content in the JSON\n",
    "    # .articles[] means: get all items from the 'articles' array\n",
    "    loader = JSONLoader(\n",
    "        file_path=json_path,\n",
    "        jq_schema=\".articles[]\",  # Extract each article\n",
    "        text_content=False  # Return full JSON for each article\n",
    "    )\n",
    "    \n",
    "    # Load articles\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} articles\\n\")\n",
    "    \n",
    "    # Inspect first article\n",
    "    print(\"üì∞ First Article:\")\n",
    "    print(f\"Content:\\n{documents[0].page_content}\\n\")\n",
    "    print(f\"Metadata: {documents[0].metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå JSON not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Extracting Specific Fields from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(json_path).exists():\n",
    "    # Extract only the article content field\n",
    "    loader = JSONLoader(\n",
    "        file_path=json_path,\n",
    "        jq_schema=\".articles[].content\",  # Get only 'content' field\n",
    "        text_content=True  # Treat as plain text\n",
    "    )\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    print(\"üìù Extracted Article Contents Only:\\n\")\n",
    "    for i, doc in enumerate(docs[:2], 1):\n",
    "        print(f\"{i}. {doc.page_content[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER TIP: Understanding jq Syntax\n",
    "\n",
    "**jq** is like a GPS for JSON:\n",
    "\n",
    "| jq Expression | Meaning |\n",
    "|--------------|----------|\n",
    "| `.` | Root of JSON |\n",
    "| `.articles` | Get the 'articles' field |\n",
    "| `.articles[]` | Get all items in 'articles' array |\n",
    "| `.articles[0]` | Get first item in 'articles' array |\n",
    "| `.articles[].title` | Get 'title' from each article |\n",
    "\n",
    "**Example:**\n",
    "```json\n",
    "{\n",
    "  \"articles\": [\n",
    "    {\"title\": \"Article 1\", \"content\": \"...\"},\n",
    "    {\"title\": \"Article 2\", \"content\": \"...\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "- `.articles[]` ‚Üí Returns both articles\n",
    "- `.articles[].title` ‚Üí Returns [\"Article 1\", \"Article 2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"html-loading\"></a>\n",
    "## 6. Loading Web Pages (HTML) üåê\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**WebBaseLoader** scrapes web pages and extracts text content.\n",
    "\n",
    "**Important:** Only works with **static HTML**. For JavaScript-rendered sites, you'd need Playwright or Selenium.\n",
    "\n",
    "### Example 1: Loading a Local HTML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load our sample blog post\n",
    "html_path = \"sample_data/blog_post.html\"\n",
    "\n",
    "if Path(html_path).exists():\n",
    "    print(f\"Loading HTML: {html_path}\\n\")\n",
    "    \n",
    "    # For local files, we need to use file:// protocol\n",
    "    file_url = f\"file://{Path(html_path).absolute()}\"\n",
    "    \n",
    "    # Create loader\n",
    "    loader = WebBaseLoader(file_url)\n",
    "    \n",
    "    # Load the page\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} document(s)\\n\")\n",
    "    \n",
    "    # Inspect content\n",
    "    doc = documents[0]\n",
    "    print(f\"üìÑ Content length: {len(doc.page_content)} characters\")\n",
    "    print(f\"\\nüìù First 500 characters:\\n{doc.page_content[:500]}...\")\n",
    "    print(f\"\\nüîç Metadata: {doc.metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå HTML not found: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Loading Multiple URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load multiple web pages at once\n",
    "# NOTE: This will actually make HTTP requests, so we're using examples\n",
    "\n",
    "# Uncomment to try with real websites:\n",
    "# urls = [\n",
    "#     \"https://python.langchain.com/docs/introduction/\",\n",
    "#     \"https://python.langchain.com/docs/expression_language/\"\n",
    "# ]\n",
    "# \n",
    "# loader = WebBaseLoader(urls)\n",
    "# docs = loader.load()\n",
    "# \n",
    "# print(f\"Loaded {len(docs)} pages\")\n",
    "# for doc in docs:\n",
    "#     print(f\"  - {doc.metadata['source']}\")\n",
    "\n",
    "print(\"üí° WebBaseLoader Example:\")\n",
    "print(\"\\nTo load web pages, use:\")\n",
    "print(\"\"\"loader = WebBaseLoader([\n",
    "    \"https://example.com/page1\",\n",
    "    \"https://example.com/page2\"\n",
    "])\"\"\")\n",
    "print(\"\\n‚ö†Ô∏è Note: Only works with static HTML (no JavaScript rendering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text-loading\"></a>\n",
    "## 7. Loading Text and Markdown Files üìù\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "For simple text files, use **TextLoader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the notes.txt file\n",
    "txt_path = \"sample_data/notes.txt\"\n",
    "\n",
    "if Path(txt_path).exists():\n",
    "    print(f\"Loading text file: {txt_path}\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    loader = TextLoader(txt_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Load the file\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} document\\n\")\n",
    "    \n",
    "    doc = documents[0]\n",
    "    print(f\"üìÑ Content length: {len(doc.page_content)} characters\")\n",
    "    print(f\"\\nüìù First 300 characters:\\n{doc.page_content[:300]}...\")\n",
    "    print(f\"\\nüîç Metadata: {doc.metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Text file not found: {txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Files\n",
    "\n",
    "For Markdown files, use **UnstructuredMarkdownLoader** (preserves structure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if README.md exists\n",
    "readme_path = \"README.md\"\n",
    "\n",
    "if Path(readme_path).exists():\n",
    "    try:\n",
    "        from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "        \n",
    "        print(f\"Loading markdown: {readme_path}\\n\")\n",
    "        \n",
    "        loader = UnstructuredMarkdownLoader(readme_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(docs)} document(s)\")\n",
    "        print(f\"\\nFirst 200 chars:\\n{docs[0].page_content[:200]}...\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è UnstructuredMarkdownLoader requires additional dependencies\")\n",
    "        print(\"   Install with: pip install unstructured\")\n",
    "        print(\"\\n   For now, using TextLoader:\")\n",
    "        \n",
    "        loader = TextLoader(readme_path)\n",
    "        docs = loader.load()\n",
    "        print(f\"   ‚úÖ Loaded with TextLoader: {len(docs[0].page_content)} chars\")\nelse:\n",
    "    print(f\"‚ÑπÔ∏è No README.md found in current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"batch-loading\"></a>\n",
    "## 8. Batch Loading with DirectoryLoader üìÇ\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**DirectoryLoader** loads all files from a directory automatically.\n",
    "\n",
    "Perfect for:\n",
    "- Loading entire document libraries\n",
    "- Processing multiple files at once\n",
    "- Building knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load all files from sample_data directory\n",
    "data_dir = \"sample_data\"\n",
    "\n",
    "if Path(data_dir).exists():\n",
    "    print(f\"üìÇ Loading all text files from: {data_dir}/\\n\")\n",
    "    \n",
    "    # Create loader for .txt files only\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=\"*.txt\",  # Pattern to match files\n",
    "        loader_cls=TextLoader,  # Use TextLoader for each file\n",
    "        show_progress=True  # Show progress bar\n",
    "    )\n",
    "    \n",
    "    # Load all matching files\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(documents)} text file(s)\\n\")\n",
    "    \n",
    "    # Show sources\n",
    "    for doc in documents:\n",
    "        print(f\"  - {doc.metadata['source']} ({len(doc.page_content)} chars)\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Loading Multiple File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Load all files from a directory (mixed types)\n",
    "# This function handles different file types intelligently\n",
    "\n",
    "def load_all_documents(directory: str) -> list:\n",
    "    \"\"\"\n",
    "    Load documents from multiple file formats in a directory.\n",
    "    \n",
    "    Supports: PDF, TXT, CSV, JSON, HTML\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    directory_path = Path(directory)\n",
    "    \n",
    "    if not directory_path.exists():\n",
    "        print(f\"‚ùå Directory not found: {directory}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÇ Loading from: {directory}\\n\")\n",
    "    \n",
    "    # Load PDFs\n",
    "    pdf_files = list(directory_path.glob(\"*.pdf\"))\n",
    "    for pdf in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  ‚úÖ PDF: {pdf.name} ({len(docs)} pages)\")\n",
    "    \n",
    "    # Load TXT files\n",
    "    txt_files = list(directory_path.glob(\"*.txt\"))\n",
    "    for txt in txt_files:\n",
    "        loader = TextLoader(str(txt))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  ‚úÖ TXT: {txt.name}\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    csv_files = list(directory_path.glob(\"*.csv\"))\n",
    "    for csv in csv_files:\n",
    "        loader = CSVLoader(str(csv))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  ‚úÖ CSV: {csv.name} ({len(docs)} rows)\")\n",
    "    \n",
    "    # Load JSON files\n",
    "    json_files = list(directory_path.glob(\"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            loader = JSONLoader(\n",
    "                str(json_file),\n",
    "                jq_schema=\".\",\n",
    "                text_content=False\n",
    "            )\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"  ‚úÖ JSON: {json_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è JSON: {json_file.name} (error: {str(e)[:50]}...)\")\n",
    "    \n",
    "    print(f\"\\nüìä Total: {len(all_docs)} documents loaded\")\n",
    "    return all_docs\n",
    "\n",
    "# Test the function\n",
    "if Path(\"sample_data\").exists():\n",
    "    all_documents = load_all_documents(\"sample_data\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    sources = [doc.metadata['source'] for doc in all_documents]\n",
    "    print(f\"   Files loaded: {len(set(sources))}\")\n",
    "    print(f\"   Total documents: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 9. Loader Comparison Table üìä\n",
    "\n",
    "### üî∞ BEGINNER REFERENCE\n",
    "\n",
    "| Loader | File Type | Use Case | Documents Created |\n",
    "|--------|-----------|----------|-------------------|\n",
    "| **PyPDFLoader** | `.pdf` | Research papers, books, reports | 1 per page |\n",
    "| **CSVLoader** | `.csv` | Product catalogs, data tables | 1 per row |\n",
    "| **JSONLoader** | `.json` | API responses, config files | Depends on jq query |\n",
    "| **WebBaseLoader** | Web URLs | Blog posts, documentation | 1 per URL |\n",
    "| **TextLoader** | `.txt` | Plain text, logs | 1 per file |\n",
    "| **UnstructuredMarkdownLoader** | `.md` | Documentation, notes | 1 per file |\n",
    "| **DirectoryLoader** | Multiple | Batch processing | All files matching pattern |\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "- üìï **Academic papers?** ‚Üí PyPDFLoader\n",
    "- üìä **Structured data?** ‚Üí CSVLoader\n",
    "- üîß **API data?** ‚Üí JSONLoader\n",
    "- üåê **Web content?** ‚Üí WebBaseLoader\n",
    "- üìù **Simple text?** ‚Üí TextLoader\n",
    "- üìÇ **Entire folder?** ‚Üí DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"best-practices\"></a>\n",
    "## 10. Best Practices üåü\n",
    "\n",
    "### üî∞ BEGINNER TIPS\n",
    "\n",
    "#### 1. Always Check File Existence\n",
    "```python\n",
    "# ‚úÖ Good\n",
    "if Path(file_path).exists():\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "# ‚ùå Bad - Will crash if file doesn't exist\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "#### 2. Use Lazy Loading for Large Files\n",
    "```python\n",
    "# For PDFs > 100 pages or files > 10MB\n",
    "for page in loader.lazy_load():\n",
    "    process_page(page)\n",
    "```\n",
    "\n",
    "#### 3. Inspect Metadata\n",
    "```python\n",
    "# Always check what metadata is available\n",
    "print(docs[0].metadata)\n",
    "```\n",
    "\n",
    "### üéì INTERMEDIATE TIPS\n",
    "\n",
    "#### 1. Error Handling\n",
    "```python\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {pdf_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {pdf_path}: {e}\")\n",
    "```\n",
    "\n",
    "#### 2. Add Custom Metadata\n",
    "```python\n",
    "# Add custom metadata after loading\n",
    "for doc in documents:\n",
    "    doc.metadata['loaded_at'] = datetime.now().isoformat()\n",
    "    doc.metadata['category'] = 'research_paper'\n",
    "```\n",
    "\n",
    "#### 3. Filter Documents\n",
    "```python\n",
    "# Filter by metadata\n",
    "research_docs = [\n",
    "    doc for doc in all_documents \n",
    "    if 'research' in doc.metadata['source'].lower()\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 11. Summary & Exercises üìù\n",
    "\n",
    "### üéâ What You Learned\n",
    "\n",
    "‚úÖ **Document Loaders** convert files into standardized Document objects\n",
    "\n",
    "‚úÖ **PyPDFLoader** loads PDF files (1 document per page)\n",
    "\n",
    "‚úÖ **CSVLoader** loads CSV data (1 document per row)\n",
    "\n",
    "‚úÖ **JSONLoader** uses jq syntax to extract data from JSON\n",
    "\n",
    "‚úÖ **WebBaseLoader** scrapes web pages (static HTML only)\n",
    "\n",
    "‚úÖ **TextLoader** handles plain text files\n",
    "\n",
    "‚úÖ **DirectoryLoader** batch processes multiple files\n",
    "\n",
    "‚úÖ All loaders return **Document** objects with `page_content` and `metadata`\n",
    "\n",
    "### üí° Practice Exercises\n",
    "\n",
    "#### üî∞ Beginner Exercises\n",
    "\n",
    "1. **Load a PDF and count pages**\n",
    "   - Use PyPDFLoader to load `attention.pdf`\n",
    "   - Print the number of pages\n",
    "   - Print the first 100 characters of page 1\n",
    "\n",
    "2. **Load CSV and find products by category**\n",
    "   - Load `products.csv`\n",
    "   - Filter documents to find only \"Electronics\"\n",
    "   - Print product names\n",
    "\n",
    "3. **Combine multiple files**\n",
    "   - Load notes.txt, products.csv, and api_response.json\n",
    "   - Count total documents\n",
    "   - Print unique sources\n",
    "\n",
    "#### üéì Intermediate Exercises\n",
    "\n",
    "1. **Build a multi-format loader**\n",
    "   - Create a function that accepts a directory path\n",
    "   - Automatically detect file types (.pdf, .csv, .json, .txt)\n",
    "   - Load all files and add custom metadata (file_type, loaded_date)\n",
    "\n",
    "2. **Extract specific data from JSON**\n",
    "   - Load `api_response.json`\n",
    "   - Use jq to extract only article titles\n",
    "   - Create a summary document with all titles\n",
    "\n",
    "3. **Lazy load and process**\n",
    "   - Use lazy_load() on a PDF\n",
    "   - Process each page and extract pages containing specific keywords\n",
    "   - Save filtered pages to a new list\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "In **Notebook 03: Text Splitting Strategies**, you'll learn how to:\n",
    "- Split long documents into chunks\n",
    "- Choose optimal chunk sizes\n",
    "- Handle overlap for better context\n",
    "- Use different splitters for different content types\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You now know how to load data from any source! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
