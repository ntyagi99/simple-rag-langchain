{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Notebook 03: Text Splitting Strategies\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand **why** text splitting is necessary for RAG\n",
    "2. Master **RecursiveCharacterTextSplitter** (the recommended default)\n",
    "3. Learn other splitters: Character, HTMLHeader, RecursiveJson, Token\n",
    "4. Choose optimal **chunk sizes** and **overlap**\n",
    "5. Compare splitters side-by-side\n",
    "6. Apply the right splitter for different content types\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Table of Contents\n",
    "\n",
    "1. [Why Split Text?](#why-split)\n",
    "2. [RecursiveCharacterTextSplitter](#recursive-splitter)\n",
    "3. [CharacterTextSplitter](#character-splitter)\n",
    "4. [HTMLHeaderTextSplitter](#html-splitter)\n",
    "5. [RecursiveJsonSplitter](#json-splitter)\n",
    "6. [TokenTextSplitter](#token-splitter)\n",
    "7. [Chunk Size & Overlap Optimization](#optimization)\n",
    "8. [Comparison & Best Practices](#comparison)\n",
    "9. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-split\"></a>\n",
    "## 1. Why Split Text? ü§î\n",
    "\n",
    "### üî∞ BEGINNER EXPLANATION\n",
    "\n",
    "Imagine you have a 200-page book and someone asks: *\"What did the author say about machine learning on page 87?\"*\n",
    "\n",
    "**Problem:** LLMs have a limited \"attention span\" (context window):\n",
    "- GPT-3.5-Turbo: ~4,000 tokens (~16,000 characters)\n",
    "- GPT-4: ~8,000 tokens (~32,000 characters)\n",
    "- You **can't** fit a whole book in one query!\n",
    "\n",
    "**Solution:** Split the book into smaller **chunks**:\n",
    "1. Each chunk is small enough for the LLM\n",
    "2. Search finds the **relevant chunks** (like page 87)\n",
    "3. Only send those chunks to the LLM\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "If you split text randomly:\n",
    "```\n",
    "‚ùå BAD SPLIT:\n",
    "Chunk 1: \"The transformer architecture revolutionized NLP. It uses self-att\"\n",
    "Chunk 2: \"ention mechanisms to process sequences in parallel. This allows...\"\n",
    "```\n",
    "\n",
    "The word \"attention\" is cut in half! üò±\n",
    "\n",
    "**Good splitters** respect boundaries (paragraphs, sentences, words):\n",
    "```\n",
    "‚úÖ GOOD SPLIT:\n",
    "Chunk 1: \"The transformer architecture revolutionized NLP. It uses self-attention mechanisms.\"\n",
    "Chunk 2: \"Self-attention allows the model to process sequences in parallel. This improves speed...\"\n",
    "```\n",
    "\n",
    "### üéì INTERMEDIATE: Trade-offs\n",
    "\n",
    "| Aspect | Small Chunks (500 chars) | Large Chunks (2000 chars) |\n",
    "|--------|-------------------------|---------------------------|\n",
    "| **Precision** | High (very specific) | Lower (more general) |\n",
    "| **Context** | Less context | More context |\n",
    "| **Retrieval Quality** | More precise matches | May include noise |\n",
    "| **# of Chunks** | More chunks = more storage | Fewer chunks |\n",
    "| **Best for** | Q&A, facts, technical docs | Long-form, narrative content |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"recursive-splitter\"></a>\n",
    "## 2. RecursiveCharacterTextSplitter ‚≠ê\n",
    "\n",
    "### üî∞ BEGINNER: The Default Choice\n",
    "\n",
    "**RecursiveCharacterTextSplitter** is your go-to splitter for 90% of cases.\n",
    "\n",
    "**How it works:**\n",
    "1. Tries to split on **double newlines** (\\n\\n) ‚Üí paragraphs\n",
    "2. If chunks still too big, splits on **single newlines** (\\n) ‚Üí lines\n",
    "3. If still too big, splits on **periods** (.) ‚Üí sentences\n",
    "4. If still too big, splits on **spaces** ( ) ‚Üí words\n",
    "5. Last resort: splits on **characters**\n",
    "\n",
    "This preserves meaning as much as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load our sample text\n",
    "txt_path = \"sample_data/notes.txt\"\n",
    "\n",
    "if Path(txt_path).exists():\n",
    "    # Load the document\n",
    "    loader = TextLoader(txt_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"üìÑ Original document: {len(documents[0].page_content)} characters\\n\")\n",
    "    \n",
    "    # Create splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Maximum chunk size in characters\n",
    "        chunk_overlap=200,      # Overlap between chunks\n",
    "        length_function=len,    # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these in order\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split into {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Examine first 3 chunks\n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Chunk {i} ({len(chunk.page_content)} chars):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(chunk.page_content[:300] + \"...\" if len(chunk.page_content) > 300 else chunk.page_content)\n",
    "        print()\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ Understanding Chunk Overlap\n",
    "\n",
    "**Why overlap?** To preserve context across boundaries.\n",
    "\n",
    "**Example without overlap:**\n",
    "```\n",
    "Chunk 1: \"...introducing the transformer architecture.\"\n",
    "Chunk 2: \"The model uses multi-head attention...\"\n",
    "```\n",
    "‚Üí Missing connection between \"transformer\" and \"multi-head attention\"\n",
    "\n",
    "**Example with overlap:**\n",
    "```\n",
    "Chunk 1: \"...introducing the transformer architecture. The model uses...\"\n",
    "Chunk 2: \"...transformer architecture. The model uses multi-head attention...\"\n",
    "```\n",
    "‚Üí Both chunks have the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overlap\n",
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "    \n",
    "    # Splitter with overlap\n",
    "    splitter_with_overlap = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100  # 100 chars overlap\n",
    "    )\n",
    "    \n",
    "    chunks = splitter_with_overlap.split_documents(docs)\n",
    "    \n",
    "    print(\"üîç Examining overlap between chunks:\\n\")\n",
    "    \n",
    "    # Show overlap between chunk 1 and 2\n",
    "    if len(chunks) >= 2:\n",
    "        chunk1_end = chunks[0].page_content[-150:]\n",
    "        chunk2_start = chunks[1].page_content[:150]\n",
    "        \n",
    "        print(\"Chunk 1 ending:\")\n",
    "        print(f\"  ...{chunk1_end}\")\n",
    "        print(\"\\nChunk 2 beginning:\")\n",
    "        print(f\"  {chunk2_start}...\")\n",
    "        print(\"\\nüí° Notice the overlap? This preserves context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Custom Separators for Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Splitting Python code\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Python code example\n",
    "python_code = '''\n",
    "def calculate_total(items):\n",
    "    \"\"\"Calculate total price of items.\"\"\"\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total += item['price']\n",
    "    return total\n",
    "\n",
    "def apply_discount(total, discount_percent):\n",
    "    \"\"\"Apply discount to total.\"\"\"\n",
    "    discount = total * (discount_percent / 100)\n",
    "    return total - discount\n",
    "\n",
    "class ShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "    \n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "'''\n",
    "\n",
    "# Python-aware splitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "code_chunks = python_splitter.split_text(python_code)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Split code into {len(code_chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(code_chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"character-splitter\"></a>\n",
    "## 3. CharacterTextSplitter\n",
    "\n",
    "### üî∞ BEGINNER: Simple Splitting\n",
    "\n",
    "**CharacterTextSplitter** splits on a single separator (like \"\\n\\n\").\n",
    "- Simpler than Recursive\n",
    "- Less intelligent\n",
    "- Use for testing or very simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Sample text with clear paragraph breaks\n",
    "sample_text = \"\"\"First paragraph about machine learning.\n",
    "It has multiple sentences. This is important context.\n",
    "\n",
    "Second paragraph about deep learning.\n",
    "Neural networks are powerful. They learn from data.\n",
    "\n",
    "Third paragraph about transformers.\n",
    "Attention mechanisms are key. They revolutionized NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Split on paragraph breaks\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on double newline (paragraphs)\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = simple_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"html-splitter\"></a>\n",
    "## 4. HTMLHeaderTextSplitter üåê\n",
    "\n",
    "### üî∞ BEGINNER: Structure-Aware Splitting\n",
    "\n",
    "**HTMLHeaderTextSplitter** splits HTML based on headers (h1, h2, h3).\n",
    "- Preserves document structure\n",
    "- Adds header information to metadata\n",
    "- Perfect for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# Load the HTML blog post\n",
    "html_path = \"sample_data/blog_post.html\"\n",
    "\n",
    "if Path(html_path).exists():\n",
    "    # Read HTML content\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Title\"),\n",
    "        (\"h2\", \"Section\"),\n",
    "        (\"h3\", \"Subsection\"),\n",
    "    ]\n",
    "    \n",
    "    # Create splitter\n",
    "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # Split the HTML\n",
    "    html_chunks = html_splitter.split_text(html_content)\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split HTML into {len(html_chunks)} sections\\n\")\n",
    "    \n",
    "    # Show first 3 sections with metadata\n",
    "    for i, chunk in enumerate(html_chunks[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Section {i}:\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content (first 200 chars): {chunk.page_content[:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"‚ùå HTML file not found: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"json-splitter\"></a>\n",
    "## 5. RecursiveJsonSplitter üì¶\n",
    "\n",
    "### üî∞ BEGINNER: Splitting JSON Data\n",
    "\n",
    "**RecursiveJsonSplitter** splits JSON while preserving structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "json_path = \"sample_data/api_response.json\"\n",
    "\n",
    "if Path(json_path).exists():\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Create splitter\n",
    "    json_splitter = RecursiveJsonSplitter(\n",
    "        max_chunk_size=1000,\n",
    "        min_chunk_size=100\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    json_chunks = json_splitter.split_text(\n",
    "        json_data=json_data,\n",
    "        convert_lists=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split JSON into {len(json_chunks)} chunks\\n\")\n",
    "    \n",
    "    # Show first chunk\n",
    "    print(\"First chunk:\")\n",
    "    print(json.dumps(json_chunks[0], indent=2)[:500] + \"...\")\n",
    "else:\n",
    "    print(f\"‚ùå JSON file not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"token-splitter\"></a>\n",
    "## 6. TokenTextSplitter üéØ\n",
    "\n",
    "### üéì INTERMEDIATE: Precise Token-Based Splitting\n",
    "\n",
    "**TokenTextSplitter** splits based on **tokens** (not characters).\n",
    "- More accurate for LLM context windows\n",
    "- Uses tiktoken (OpenAI's tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"The transformer architecture, introduced in the paper 'Attention Is All You Need', \n",
    "revolutionized natural language processing. It uses self-attention mechanisms to process \n",
    "sequences in parallel, making it much faster than recurrent neural networks.\"\"\"\n",
    "\n",
    "# Token-based splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,  # 50 tokens (not characters!)\n",
    "    chunk_overlap=10,\n",
    "    encoding_name=\"cl100k_base\"  # GPT-3.5/GPT-4 tokenizer\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "\n",
    "print(f\"Split into {len(token_chunks)} token-based chunks:\\n\")\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optimization\"></a>\n",
    "## 7. Chunk Size & Overlap Optimization üìä\n",
    "\n",
    "### üî∞ BEGINNER: Rules of Thumb\n",
    "\n",
    "#### Recommended Configurations\n",
    "\n",
    "| Content Type | Chunk Size | Overlap | Why |\n",
    "|-------------|-----------|---------|-----|\n",
    "| **General Text** | 1000 chars | 200 chars | Balanced precision & context |\n",
    "| **Technical Docs** | 500-800 | 100-150 | Precision for code/commands |\n",
    "| **Long Articles** | 1500-2000 | 300 | More context for narrative |\n",
    "| **Code** | 200-400 | 50-100 | Function/class level |\n",
    "| **FAQs** | 200-300 | 30-50 | Question-answer pairs |\n",
    "\n",
    "#### Overlap Guidelines\n",
    "- **10-15%**: Minimal overlap, saves storage\n",
    "- **20%**: Sweet spot (recommended)\n",
    "- **30%+**: Maximum context preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chunk sizes\n",
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "    \n",
    "    chunk_sizes = [500, 1000, 1500, 2000]\n",
    "    \n",
    "    print(\"üìä Chunk Size Comparison:\\n\")\n",
    "    print(f\"{'Size':<8} {'Chunks':<10} {'Avg Length':<12} {'Overlap %'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        overlap = int(size * 0.2)  # 20% overlap\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        avg_length = sum(len(c.page_content) for c in chunks) / len(chunks)\n",
    "        overlap_pct = (overlap / size) * 100\n",
    "        \n",
    "        print(f\"{size:<8} {len(chunks):<10} {avg_length:<12.0f} {overlap_pct:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 8. Comparison & Best Practices üåü\n",
    "\n",
    "### Splitter Comparison\n",
    "\n",
    "| Splitter | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **RecursiveCharacter** | General text, docs | Smart boundaries, flexible | Slower |\n",
    "| **Character** | Simple text | Fast, simple | Not intelligent |\n",
    "| **HTMLHeader** | Web content, docs | Preserves structure | HTML only |\n",
    "| **RecursiveJson** | JSON data | Preserves JSON structure | JSON only |\n",
    "| **Token** | Precise LLM usage | Accurate token count | Requires tokenizer |\n",
    "\n",
    "### üéì Best Practices\n",
    "\n",
    "1. **Start with RecursiveCharacterTextSplitter**\n",
    "2. **Test different chunk sizes** with your data\n",
    "3. **Use 20% overlap** as default\n",
    "4. **Match splitter to content type** (HTML ‚Üí HTMLHeaderTextSplitter)\n",
    "5. **Monitor retrieval quality** and adjust\n",
    "6. **Consider token-based splitting** for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 9. Summary & Exercises üìù\n",
    "\n",
    "### üéâ What You Learned\n",
    "\n",
    "‚úÖ Text splitting is necessary because **LLMs have limited context windows**\n",
    "\n",
    "‚úÖ **RecursiveCharacterTextSplitter** is the recommended default\n",
    "\n",
    "‚úÖ **Chunk size** determines precision vs context trade-off\n",
    "\n",
    "‚úÖ **Overlap** (20%) preserves context across boundaries\n",
    "\n",
    "‚úÖ Different content types need different splitters\n",
    "\n",
    "‚úÖ **Best practice:** chunk_size=1000, chunk_overlap=200 for general text\n",
    "\n",
    "### üí° Practice Exercises\n",
    "\n",
    "#### üî∞ Beginner\n",
    "1. Load a PDF and split it with chunk_size=500, overlap=100\n",
    "2. Count total chunks created\n",
    "3. Print first and last chunks\n",
    "\n",
    "#### üéì Intermediate\n",
    "1. Compare chunk sizes (500, 1000, 2000) on the same document\n",
    "2. Create a chart showing # of chunks vs chunk size\n",
    "3. Test different overlap percentages (10%, 20%, 30%)\n",
    "\n",
    "### üìö Next: Notebook 04 - Embeddings!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
